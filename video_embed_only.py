#!/usr/bin/env python3
"""
video_embed_only.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â€¢ ì„¸ê·¸ë¨¼íŠ¸ í”„ë ˆì„ ì—†ìœ¼ë©´ ffmpeg ë¡œ ì¶”ì¶œ
  â€¢ BEiT-base + TemporalAttention ìœ¼ë¡œ 768-d ë¹„ë””ì˜¤ ì„ë² ë”©
  â€¢ GPU ê°¯ìˆ˜ë§Œí¼ í”„ë¡œì„¸ìŠ¤ ìƒì„±, ì˜ìƒ(video_id) ë‹¨ìœ„ë¡œ ìˆœì°¨ ì²˜ë¦¬
ì‚¬ìš© ì˜ˆ)
CUDA_VISIBLE_DEVICES=0,1,2,3 python video_embed_only.py \
    --cache ./cache --workers 64
"""
from __future__ import annotations
import os, sqlite3, subprocess, argparse, multiprocessing as mp, logging, warnings
from pathlib import Path
from typing import Dict, List, Tuple
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm
import torch                                         # ë©”ì¸ í”„ë¡œì„¸ìŠ¤

# â”€â”€â”€â”€â”€ ê²½ë¡œ/íŒŒë¼ë¯¸í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DB_PATH        = "data/speech_segments.db"
RAW_VIDEO_DIR  = Path("/mnt/shares/videos")
FRAME_OUT_ROOT = Path("/mnt/third_ssd/data/frames")
FPS            = 1

# â”€â”€â”€â”€â”€ ë¡œê·¸/ì›Œë‹ ìµœì†Œí™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
warnings.filterwarnings("ignore")
os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "true"
os.environ["TRANSFORMERS_VERBOSITY"]            = "error"
for n in ["transformers","urllib3","accelerate","huggingface_hub",
          "numba","requests","PIL"]:
    logging.getLogger(n).setLevel(logging.ERROR)

# ---------------- ffmpeg ------------------------------------------
def run_ffmpeg(cmd: List[str]):
    subprocess.run(cmd, check=True,
                   stderr=subprocess.PIPE,
                   stdout=subprocess.DEVNULL)

# --------------- ì„¸ê·¸ë¨¼íŠ¸ ì‘ì—… (CPU) -------------------------------
def process_segment(seg: Tuple[str,str,float,float],
                    cache_dir: Path) -> str|None:
    """
    seg = (segment_id, video_id, start, end)
    ì„±ê³µ âœ None, ì‹¤íŒ¨ âœ str(ë©”ì‹œì§€)
    """
    seg_id, vid, st, et = seg
    video_file = RAW_VIDEO_DIR / f"{vid}.mp4"
    if not video_file.exists():
        return f"{seg_id}: video_missing"

    frame_dir = FRAME_OUT_ROOT / vid / seg_id
    if not (frame_dir.exists() and any(frame_dir.iterdir())):
        frame_dir.mkdir(parents=True, exist_ok=True)
        try:
            run_ffmpeg([
                "ffmpeg", "-nostdin", "-loglevel", "error", "-y",
                "-ss", str(st), "-to", str(et), "-i", str(video_file),
                "-vf", f"fps={FPS}", str(frame_dir/"f%06d.jpg")
            ])
        except subprocess.CalledProcessError as e:
            return f"{seg_id}: ffmpeg_fail {e}"

    # ì„ë² ë”© (GPU ëª¨ë¸ì€ ìƒìœ„ í”„ë¡œì„¸ìŠ¤ì— ì´ë¯¸ ì˜¬ë¼ê°€ ìˆìŒ)
    from embed_utils import embed_video
    try:
        embed_video(frame_dir, cache_dir, seg_id)
    except Exception as e:
        return f"{seg_id}: embed_fail {e}"
    return None

# --------------- GPU í”„ë¡œì„¸ìŠ¤ -------------------------------------
def gpu_worker(gid:int,
               rows_slice:List[Tuple[str,str,float,float]],
               cache_dir:Path,
               cpu_threads:int):
    os.environ["CUDA_VISIBLE_DEVICES"] = str(gid)
    from embed_utils import embed_video   # BEiT ë¡œë“œ (1íšŒ)

    # video_id â†’ [ì„¸ê·¸â€¦] ê·¸ë£¹í•‘
    vids:Dict[str, List[Tuple]] = {}
    for seg in rows_slice:
        vids.setdefault(seg[1], []).append(seg)

    print(f"[GPU{gid}] videos={len(vids)}  segs={len(rows_slice)}")

    for vid_idx,(vid,segs) in enumerate(vids.items(),1):
        desc = f"GPU{gid} {vid} ({vid_idx}/{len(vids)})"
        fails = 0
        with ProcessPoolExecutor(max_workers=cpu_threads) as ex:
            futs = [ex.submit(process_segment,s,cache_dir) for s in segs]
            for fut in tqdm(as_completed(futs),
                            total=len(futs),desc=desc,ncols=95,leave=False):
                err = fut.result()
                if err:
                    fails += 1
                    tqdm.write("âš  "+err)
        ok = len(segs)-fails
        print(f"[GPU{gid}] {vid}  {ok}/{len(segs)} done  fails={fails}")

    print(f"[GPU{gid}] finished all videos")

# ----------------------------- ë©”ì¸ -------------------------------
def main():
    pa = argparse.ArgumentParser()
    pa.add_argument("--cache", default="./cache")
    pa.add_argument("--workers",type=int,default=os.cpu_count())
    args = pa.parse_args()
    cache_dir = Path(args.cache).resolve(); cache_dir.mkdir(exist_ok=True)

    # DB ì½ê¸°
    conn = sqlite3.connect(DB_PATH); conn.row_factory = sqlite3.Row
    rows = [(r["segment_id"], r["video_id"],
             r["start_time"], r["end_time"])
            for r in conn.execute(
            "SELECT segment_id,video_id,start_time,end_time FROM speech_segments")]
    conn.close()
    if not rows: raise SystemExit("âŒ DBê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤")

    gpus = torch.cuda.device_count()
    rows_per_gpu = (len(rows)+gpus-1)//gpus
    cpu_per_gpu  = max(1, args.workers//gpus)

    procs=[]
    for gid in range(gpus):
        subset = rows[gid*rows_per_gpu:(gid+1)*rows_per_gpu]
        p = mp.Process(target=gpu_worker,
                       args=(gid,subset,cache_dir,cpu_per_gpu))
        p.start(); procs.append(p)
        print(f"Spawn GPU{gid}: segs={len(subset)} cpu={cpu_per_gpu}")

    for p in procs: p.join()
    print("ğŸ‰  video embedding completed")

if __name__ == "__main__":
    mp.set_start_method("spawn",force=True)
    main()
