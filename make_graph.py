# import pandas as pd
# import sqlite3
# import torch
# import hashlib
# import json
# import re
# from functools import lru_cache
# from pathlib import Path
# from typing import Dict, List, Tuple
# from torch_geometric.data import Data
# from transformers import BertTokenizerFast, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
# import torch.nn as nn
# import os
# import sys

# # GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸ ë° ì„¤ì •
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# print(f"Using device: {device}")

# # í•´ì‹œ í•¨ìˆ˜ ì •ì˜
# def _hash(key: str) -> str:
#     return hashlib.md5(key.encode()).hexdigest() + ".pt"

# # Tensorë¥¼ ì§€ì •ëœ ë””ë°”ì´ìŠ¤ë¡œ ì˜®ê¸°ëŠ” í—¬í¼ í•¨ìˆ˜
# def _to_device(data: Dict, device: torch.device) -> Dict:
#     return {k: v.to(device) for k, v in data.items()}

# # VRAM ë¡œê¹… í•¨ìˆ˜
# def log_vram(stage: str, device: torch.device):
#     if device.type == 'cuda':
#         torch.cuda.synchronize()
#         dev_id = device.index if isinstance(device, torch.device) else torch.cuda.current_device()
#         alloc = torch.cuda.memory_allocated(dev_id) / 1024**2
#         reserved = torch.cuda.memory_reserved(dev_id) / 1024**2
#         peak_a = torch.cuda.max_memory_allocated(dev_id) / 1024**2
#         peak_r = torch.cuda.max_memory_reserved(dev_id) / 1024**2
#         wasted = reserved - alloc
#         print(f"[{stage:15s}] alloc: {alloc:6.1f} MB | reserved: {reserved:6.1f} MB | "
#               f"peak_alloc: {peak_a:6.1f} MB | peak_reserved: {peak_r:6.1f} MB | wasted: {wasted:6.1f} MB")
#         torch.cuda.reset_peak_memory_stats(dev_id)

# # Edge type id ì •ì˜
# EDGE_TYPE: Dict[str, int] = {
#     "t_t": 0,  # text  â†” text
#     "v_v": 1,  # video â†” video
#     "a_a": 2,  # audio â†” audio
#     "t_v": 3,  # text  â†” video
#     "t_a": 4,  # text  â†” audio
#     "utt": 5,  # utterance â†” utterance (for merging)
# }

# # ì–‘ë°©í–¥ ì—£ì§€ ì¶”ê°€ í—¬í¼
# def _add_bidir(
#     src: int,
#     dst: int,
#     etype: int,
#     edge_src: List[int],
#     edge_dst: List[int],
#     edge_type: List[int],
# ):
#     edge_src.extend([src, dst])
#     edge_dst.extend([dst, src])
#     edge_type.extend([etype, etype])
    
# # models/graph_builder.pyì˜ ì˜ì¡´ì„± í•´ê²°ì„ ìœ„í•´ í•„ìš”í•œ í´ë˜ìŠ¤ë“¤ì„ ì—¬ê¸°ì— í¬í•¨
# class ExternalFinancialKnowledgeModel:
#     def __init__(self,
#                  wiki_db="data/wikidata_revisions.db",
#                  speech_db="data/speech_segments.db"):
#         self.wiki = sqlite3.connect(wiki_db); self.wiki.row_factory = sqlite3.Row
#         self.speech = sqlite3.connect(speech_db); self.speech.row_factory = sqlite3.Row

#         self.target_entities = self._load_target_entities()
#         label_df = pd.read_sql("SELECT DISTINCT qid,value AS label FROM labels "
#                                "WHERE lang='en';", self.wiki)
#         label_df = label_df[label_df["label"].str.lower()
#                             .isin([n.lower() for n in self.target_entities])]
#         label_df["id_int"] = pd.factorize(label_df["qid"])[0]

#         self.qid2int = dict(zip(label_df["qid"], label_df["id_int"]))
#         self.int2qid = {v: k for k, v in self.qid2int.items()}  # ìˆ˜ì •ëœ ë¶€ë¶„ ë°˜ì˜
#         self.label2qid = {l.lower(): q for q, l in zip(label_df["qid"],
#                                                      label_df["label"])}

#         safe = [re.sub(r"\s+", r"\\s+", re.escape(n.lower()))
#                 for n in self.target_entities]
#         self._pat = re.compile(r"(" + "|".join(safe) + r")", re.I)

#     def _load_target_entities(self) -> List[str]:
#         df = pd.read_sql("SELECT persons_found FROM video_metadata;", self.speech)
#         names = set()
#         for js in df["persons_found"]:
#             if js: names.update(json.loads(js).keys())
#         return list(names)

#     def identify_entities(self, text: str) -> List[str]:
#         t = re.sub(r"[^\w\s]", "", text.lower())
#         return list({m.strip() for m in self._pat.findall(t)})

#     def entities_to_id(self, ents: List[str]) -> List[int]:
#         return [self.qid2int[self.label2qid[e.lower()]]
#                 for e in ents if e.lower() in self.label2qid]
    
#     @lru_cache(maxsize=32)
#     def _graph_until(self, time_iso: str) -> Data:
#         sql = ("SELECT c.qid subj,c.property pid,c.value_qid obj "
#                "FROM claims c JOIN revisions r USING(qid,revision_id) "
#                "WHERE r.timestamp<=?")
#         df = pd.read_sql(sql, self.wiki, params=(time_iso,))
#         df = df[df["subj"].isin(self.qid2int) & df["obj"].isin(self.qid2int)]
#         if df.empty: return Data()

#         src = torch.tensor(df["subj"].map(self.qid2int).to_numpy(), dtype=torch.long)
#         dst = torch.tensor(df["obj"].map(self.qid2int).to_numpy(), dtype=torch.long)
#         rel = torch.tensor(pd.factorize(df["pid"])[0], dtype=torch.long).view(-1, 1)
#         return Data(edge_index=torch.stack([src, dst]), edge_attr=rel)
        
#     def acquire_related_external_knowledge(
#         self, text: str, time_iso: str,
#         add_reverse=True, add_self_loop=True
#     ) -> Tuple[List[int], Data]:
#         ids = self.entities_to_id(self.identify_entities(text))
#         G = self._graph_until(time_iso)
#         if not ids or G.edge_index.numel() == 0: return ids, Data()

#         mask = (torch.isin(G.edge_index[0], torch.tensor(ids)) |
#                 torch.isin(G.edge_index[1], torch.tensor(ids)))
#         ei, ea = G.edge_index[:, mask], G.edge_attr[mask]

#         if add_reverse:
#             ei = torch.cat([ei, ei.flip(0)], 1)
#             ea = torch.cat([ea, ea], 0)
#         if add_self_loop:
#             loops = torch.tensor(ids, dtype=torch.long, device=ei.device)
#             ei = torch.cat([ei, loops.unsqueeze(0).repeat(2, 1)], 1)
#             ea = torch.cat([ea, torch.full((len(loops), 1), -1,
#                                          dtype=torch.long, device=ei.device)], 0)
#         return ids, Data(edge_index=ei, edge_attr=ea)

# # TextFeatureExtractor í´ë˜ìŠ¤ (ìˆ˜ì •ëœ ë‚´ìš© ë°˜ì˜)
# def build_cross_map(wp_offsets: List[Tuple[int, int]],
#                     glm_offsets: List[Tuple[int, int]]) -> List[List[int]]:
#     mapping = [[] for _ in wp_offsets]; p = 0
#     for i,(ws,we) in enumerate(wp_offsets):
#         while p < len(glm_offsets) and glm_offsets[p][1] <= ws: p += 1
#         q = p
#         while q < len(glm_offsets) and glm_offsets[q][0] < we:
#             mapping[i].append(q); q += 1
#     return mapping

# class TextFeatureExtractor:
#     def __init__(self):
#         self.wp_tok = BertTokenizerFast.from_pretrained("bert-base-uncased")
#         self.glm_tok: AutoTokenizer = AutoTokenizer.from_pretrained(
#             "THUDM/chatglm2-6b", trust_remote_code=True
#         )
#         self.fast_offset = True
#         quantization_config = BitsAndBytesConfig(load_in_8bit=True)
#         self.glm = AutoModelForCausalLM.from_pretrained(
#             "THUDM/chatglm2-6b",
#             trust_remote_code=True,
#             quantization_config=quantization_config,
#             output_hidden_states=True,
#             device_map="auto"
#         )
#         self.proj_down = nn.Linear(4096, 768, bias=False).to(device).half()
#         self.proj_up = nn.Linear(768, 4096, bias=False).to(device).half()
#         self.norm = nn.LayerNorm(768).to(device).half()

#         for m in (self.proj_down, self.proj_up):
#             nn.init.xavier_uniform_(m.weight)

#     def _manual_offsets(self, text: str, toks: List[str]):
#         norm = text.lower(); p, off = 0, []
#         for t in toks:
#             tc = t.lstrip(" "); tc = tc if tc else " "
#             j = norm.find(tc, p)
#             j = j if j != -1 else p
#             off.append((j, j + len(tc))); p = j + len(tc)
#         return off

#     @torch.no_grad()
#     def encode(self,
#                utterance_text: str,
#                knowledge_triples: list | None = None,
#                anchor_entities: list | None = None):
#         know_parts = []
#         if anchor_entities:
#             know_parts.append(" [ENT] ".join(map(str, anchor_entities)))
#         if knowledge_triples:
#             know_parts += [f"{h} [R] {r} [T] {t}" for h, r, t in knowledge_triples]
#         knowledge_text = " ".join(know_parts) if know_parts else None

#         wp = self.wp_tok(
#             utterance_text,
#             text_pair=knowledge_text,
#             return_offsets_mapping=True,
#             return_token_type_ids=True,
#             return_tensors="pt",
#         )
#         wp = _to_device(wp, self.glm.device)
#         wp_tokens = self.wp_tok.convert_ids_to_tokens(wp["input_ids"][0])
#         wp_offsets = wp["offset_mapping"][0].tolist()
#         token_types = wp["token_type_ids"][0].tolist()
#         sep_idx = [i for i, tok in enumerate(wp_tokens) if tok == "[SEP]"]

#         knowledge_blocks: List[Tuple[int, int]] = []
#         in_block, start = False, None
#         for i, (tt, tok) in enumerate(zip(token_types, wp_tokens)):
#             if tt == 1 and tok != "[SEP]":
#                 if not in_block:
#                     start, in_block = i, True
#             else:
#                 if in_block:
#                     knowledge_blocks.append((start, i))
#                     in_block = False
#         if in_block:
#             knowledge_blocks.append((start, len(wp_tokens)))

#         merged = utterance_text + (" [SEP] " + knowledge_text if knowledge_text else "")
#         glm_ids = self.glm_tok.encode(merged, add_special_tokens=False)
#         glm_enc = {
#             "input_ids": torch.tensor([glm_ids], device=self.glm.device),
#             "attention_mask": torch.ones(
#                 1, len(glm_ids), dtype=torch.long, device=self.glm.device)
#         }
#         glm_tokens = self.glm_tok.convert_ids_to_tokens(glm_ids)
#         glm_offsets = (self.glm_tok(
#             merged, add_special_tokens=False, return_offsets_mapping=True
#         )["offset_mapping"]
#                        if hasattr(self.glm_tok, "get_offsets_mapping")
#                        else self._manual_offsets(merged, glm_tokens))

#         hidden4096 = self.glm(**glm_enc).hidden_states[-1][0]
#         hid768 = self.norm(self.proj_down(hidden4096))

#         map_wp2glm = build_cross_map(wp_offsets, glm_offsets)
#         max_idx = hid768.size(0)
#         wp_emb = torch.stack([
#             (
#                 hid768[torch.tensor(valid, dtype=torch.long,
#                                     device=hid768.device)].mean(0)
#                 if (valid := [i for i in ids if i < max_idx])
#                 else torch.zeros(768, device=hid768.device)
#             )
#             for ids in map_wp2glm
#         ])

#         meta = {
#             "wp_tokens": wp_tokens,
#             "glm_tokens": glm_tokens,
#             "map_wp2glm": map_wp2glm,
#             "sep": sep_idx,
#             "knowledge_blocks": knowledge_blocks,
#         }
#         return wp_emb, meta

#     def cleanup(self):
#         print("TextFeatureExtractor ë©”ëª¨ë¦¬ í•´ì œ ì¤‘...")
#         if hasattr(self, 'glm'):
#             del self.glm
#         if hasattr(self, 'proj_down'):
#             del self.proj_down
#         if hasattr(self, 'proj_up'):
#             del self.proj_up
#         torch.cuda.empty_cache()
#         print("TextFeatureExtractor ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
# # GraphBuilder í´ë˜ìŠ¤
# class GraphBuilder:
#     """ë‹¨ì¼ ë°œí™” ê·¸ë˜í”„ + ì—¬ëŸ¬ ë°œí™” ë³‘í•© ìœ í‹¸ (***ì–‘ë°©í–¥ ì—£ì§€ ë²„ì „***)"""

#     def __init__(self, time_iso: str, *, merge_anchor: bool = False):
#         self.time_iso = time_iso
#         self.merge_anchor = merge_anchor
#         self.text_enc = TextFeatureExtractor()
#         self.ekm = ExternalFinancialKnowledgeModel()

#     def __del__(self):
#         if hasattr(self, 'text_enc'):
#             self.text_enc.cleanup()
#             del self.text_enc
#         print("GraphBuilder ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
#         log_vram("del", device)

#     def _triple_to_string(self, subj: str, pid: str, obj: str) -> str:
#         if not hasattr(self, "_pid_cache"):
#             self._pid_cache: Dict[str, str] = {}
#         if pid not in self._pid_cache:
#             row = self.ekm.wiki.execute(
#                 "SELECT label FROM property_labels WHERE pid=? LIMIT 1", (pid,)
#             ).fetchone()
#             self._pid_cache[pid] = row[0] if row else pid
#         return f"{subj} [REL] {self._pid_cache[pid].replace(' ', '_')} [REL] {obj}"

#     def build(
#         self,
#         utterance_text: str,
#         video_emb: torch.Tensor | None = None,
#         audio_emb: torch.Tensor | None = None,
#     ) -> Data:
#         """ë°œí™”(í…ìŠ¤íŠ¸+ëª¨ë‹¬) â†’ PyG Data (ëª¨ë“  ì—£ì§€ ì–‘ë°©í–¥)"""
#         if video_emb is not None and video_emb.is_cuda:
#             video_emb = video_emb.cpu()
#         if audio_emb is not None and audio_emb.is_cuda:
#             audio_emb = audio_emb.cpu()

#         # 1) ì™¸ë¶€ ì§€ì‹ íŠ¸ë¦¬í”Œ ìˆ˜ì§‘
#         ent_ids, ek_sub = self.ekm.acquire_related_external_knowledge(
#             utterance_text, self.time_iso
#         )
#         triples: List[Tuple[str, str, str]] = []
#         if ek_sub.num_edges:
#             for s, d, (prop,) in zip(*ek_sub.edge_index, ek_sub.edge_attr):
#                 triples.append(
#                     (
#                         self.ekm.int2qid[s.item()],
#                         f"P{prop.item()}",
#                         self.ekm.int2qid[d.item()],
#                     )
#                 )

#         # 2) í…ìŠ¤íŠ¸ ì¸ì½”ë”©
#         wp_emb, meta = self.text_enc.encode(
#             utterance_text,
#             knowledge_triples=[self._triple_to_string(*t) for t in triples],
#             anchor_entities=[self.ekm.int2qid[i] for i in ent_ids],
#         )
#         if wp_emb.is_cuda:
#             wp_emb = wp_emb.cpu()
#         hs = wp_emb
#         D = hs.size(1)
#         sep0 = meta["sep"][0] if meta["sep"] else -1

#         node_feats: List[torch.Tensor] = []
#         edge_src: List[int] = []
#         edge_dst: List[int] = []
#         edge_type: List[int] = []

#         # --- utterance text í† í° ë…¸ë“œ ---
#         text_token_map: Dict[int, int] = {
#             idx: len(node_feats) for idx in range(1, sep0)
#         }
#         node_feats.extend([hs[i] for i in range(1, sep0)])
#         text_nodes = list(text_token_map.values())
#         for i in range(len(text_nodes) - 1):
#             _add_bidir(
#                 text_nodes[i], text_nodes[i + 1], EDGE_TYPE["t_t"], edge_src, edge_dst, edge_type
#             )

#         # --- knowledge í† í° ë…¸ë“œ ---
#         knowledge_token_map: Dict[int, int] = {}
#         for s, e in meta["knowledge_blocks"]:
#             prev = None
#             for idx in range(s, e):
#                 g = len(node_feats)
#                 knowledge_token_map[idx] = g
#                 node_feats.append(hs[idx])
#                 if prev is not None:
#                     _add_bidir(prev, g, EDGE_TYPE["t_t"], edge_src, edge_dst, edge_type)
#                 prev = g

#         # ë¹„ë””ì˜¤ ë…¸ë“œ
#         if video_emb is not None and video_emb.numel():
#             v_idx = len(node_feats)
#             node_feats.append(video_emb.squeeze(0))
#             for t in text_nodes:
#                 _add_bidir(t, v_idx, EDGE_TYPE["t_v"], edge_src, edge_dst, edge_type)
#         else:
#             v_idx = -1

#         # ì˜¤ë””ì˜¤ ë…¸ë“œ
#         if audio_emb is not None and audio_emb.numel():
#             a_idx = len(node_feats)
#             node_feats.append(audio_emb.squeeze(0))
#             for t in text_nodes:
#                 _add_bidir(t, a_idx, EDGE_TYPE["t_a"], edge_src, edge_dst, edge_type)
#         else:
#             a_idx = -1

#         x = torch.stack(node_feats) if node_feats else torch.empty(0, D)
#         edge_index = (
#             torch.tensor([edge_src, edge_dst], dtype=torch.long)
#             if edge_src
#             else torch.empty(2, 0, dtype=torch.long)
#         )
#         edge_type_t = (
#             torch.tensor(edge_type, dtype=torch.long)
#             if edge_type
#             else torch.empty(0, dtype=torch.long)
#         )

#         data = Data(x=x, edge_index=edge_index, edge_type=edge_type_t)
#         data.node_meta = {
#             "text_nodes": len(text_nodes),
#             "knowledge_nodes": len(knowledge_token_map),
#             "video_nodes": 1 if v_idx != -1 else 0,
#             "audio_nodes": 1 if a_idx != -1 else 0,
#             "triples": len(triples),
#         }
#         data.utt_meta = {
#             "first_text_node": text_nodes[0] if text_nodes else -1,
#             "last_text_node": text_nodes[-1] if text_nodes else -1,
#         }
#         return data

# # models/graph_utils.pyì˜ merge_graph ë©”ì†Œë“œ
# def merge_graph(prev_graph: Data | None, current_graph: Data) -> Data:
#     if prev_graph is None or prev_graph.x.numel() == 0:
#         return current_graph

#     # ë…¸ë“œ í”¼ì²˜ ë³‘í•©
#     x_merged = torch.cat([prev_graph.x, current_graph.x], dim=0)

#     # ì—£ì§€ ì¸ë±ìŠ¤ ë° íƒ€ì… ë³‘í•© (ì˜¤í”„ì…‹ ì ìš©)
#     num_prev_nodes = prev_graph.x.size(0)
#     edge_index_current_offset = current_graph.edge_index + num_prev_nodes
#     edge_index_merged = torch.cat([prev_graph.edge_index, edge_index_current_offset], dim=1)

#     edge_type_merged = torch.cat([prev_graph.edge_type, current_graph.edge_type])

#     # utterance ì—°ê²° ì—£ì§€ ì¶”ê°€ (ì–‘ë°©í–¥)
#     if prev_graph.utt_meta["last_text_node"] != -1 and current_graph.utt_meta["first_text_node"] != -1:
#         prev_last_node = prev_graph.utt_meta["last_text_node"]
#         curr_first_node = current_graph.utt_meta["first_text_node"] + num_prev_nodes
        
#         utt_edge_index = torch.tensor([[prev_last_node, curr_first_node],
#                                        [curr_first_node, prev_last_node]], dtype=torch.long)
#         utt_edge_type = torch.tensor([EDGE_TYPE["utt"], EDGE_TYPE["utt"]], dtype=torch.long)

#         edge_index_merged = torch.cat([edge_index_merged, utt_edge_index], dim=1)
#         edge_type_merged = torch.cat([edge_type_merged, utt_edge_type])

#     # ë©”íƒ€ ì •ë³´ ë³‘í•©
#     node_meta_merged = {k: prev_graph.node_meta.get(k, 0) + current_graph.node_meta.get(k, 0)
#                         for k in set(prev_graph.node_meta.keys()) | set(current_graph.node_meta.keys())}
    
#     utt_meta_merged = {
#         "first_text_node": prev_graph.utt_meta["first_text_node"],
#         "last_text_node": current_graph.utt_meta["last_text_node"] + num_prev_nodes
#     }

#     merged_graph = Data(x=x_merged, edge_index=edge_index_merged, edge_type=edge_type_merged)
#     merged_graph.node_meta = node_meta_merged
#     merged_graph.utt_meta = utt_meta_merged
#     return merged_graph

# def build_and_cache_graphs():
#     """
#     ready_videos.csvì— ìˆëŠ” video_idì— ëŒ€í•´ ê·¸ë˜í”„ë¥¼ êµ¬ì¶•í•˜ê³  ìºì‹œë¡œ ì €ì¥í•©ë‹ˆë‹¤.
#     """
#     # íŒŒì¼ ê²½ë¡œ ì„¤ì •
#     ready_videos_path = "data/ready_videos.csv"
#     speech_db_path = "data/speech_segments.db"
#     cache_dir = Path("cache")
#     cache_dir.mkdir(exist_ok=True)

#     # 1. ready_videos.csvì—ì„œ video_id ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
#     try:
#         ready_videos_df = pd.read_csv(ready_videos_path)
#         video_ids = ready_videos_df["video_id"].tolist()
#         print(f"ì´ {len(video_ids)}ê°œì˜ video_idë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
#     except FileNotFoundError:
#         print(f"ì˜¤ë¥˜: {ready_videos_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
#         return

#     # 2. speech_segments.db ì—°ê²°
#     try:
#         conn = sqlite3.connect(speech_db_path)
#         conn.row_factory = sqlite3.Row
#         print(f"{speech_db_path}ì— ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
#     except sqlite3.Error as e:
#         print(f"ì˜¤ë¥˜: {speech_db_path} ì—°ê²° ì‹¤íŒ¨ - {e}")
#         return

#     # 3. ê° video_idì— ëŒ€í•´ ê·¸ë˜í”„ êµ¬ì¶• ë° ìºì‹œ ì €ì¥
#     for video_id in video_ids:
#         print(f"\n[Video ID: {video_id}] ê·¸ë˜í”„ êµ¬ì¶• ì‹œì‘...")
        
#         # ë¹„ë””ì˜¤ ì—…ë¡œë“œ ì‹œê°„ ì¡°íšŒ
#         try:
#             video_meta = conn.execute(
#                 "SELECT published_date FROM video_metadata WHERE video_id = ?",
#                 (video_id,)
#             ).fetchone()
#             if not video_meta:
#                 print(f"ê²½ê³ : video_id {video_id}ì— ëŒ€í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
#                 continue
#             upload_time = video_meta["published_date"]
#         except sqlite3.Error as e:
#             print(f"ì˜¤ë¥˜: video_id {video_id}ì˜ ì—…ë¡œë“œ ì‹œê°„ ì¡°íšŒ ì‹¤íŒ¨ - {e}")
#             continue

#         # GraphBuilder ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
#         try:
#             graph_builder = GraphBuilder(time_iso=upload_time)
#         except Exception as e:
#             print(f"ì˜¤ë¥˜: GraphBuilder ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨ - {e}")
#             continue

#         video_graph = None

#         # ë¹„ë””ì˜¤ì— ëŒ€í•œ ë°œí™” ë¶ˆëŸ¬ì˜¤ê¸°
#         speech_segments = conn.execute(
#             "SELECT segment_id, script FROM speech_segments WHERE video_id = ? ORDER BY start_time",
#             (video_id,)
#         ).fetchall()
        
#         if not speech_segments:
#             print(f"ê²½ê³ : video_id {video_id}ì— ëŒ€í•œ ë°œí™”ê°€ ì—†ìŠµë‹ˆë‹¤. ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
#             continue
        
#         for segment in speech_segments:
#             seg_id = segment["segment_id"]
#             utterance_text = segment["script"]
#             print(f" - ë°œí™” '{seg_id}' ì²˜ë¦¬ ì¤‘...")

#             # ìºì‹œ íŒŒì¼ ê²½ë¡œ ì„¤ì •
#             video_key = f"vid::{seg_id}"
#             audio_key = f"aud::{seg_id}"
#             video_cache_path = cache_dir / _hash(video_key)
#             audio_cache_path = cache_dir / _hash(audio_key)

#             # ì„ë² ë”© í…ì„œ ë¡œë“œ
#             video_emb, audio_emb = None, None
#             try:
#                 if video_cache_path.exists():
#                     video_emb = torch.load(video_cache_path)
#                 if audio_cache_path.exists():
#                     audio_emb = torch.load(audio_cache_path)
#             except Exception as e:
#                 print(f"ì˜¤ë¥˜: ë°œí™” '{seg_id}'ì˜ ìºì‹œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ - {e}")
#                 continue

#             # ë‹¨ì¼ ë°œí™” ê·¸ë˜í”„ êµ¬ì¶•
#             try:
#                 current_graph = graph_builder.build(
#                     utterance_text=utterance_text,
#                     video_emb=video_emb,
#                     audio_emb=audio_emb
#                 )
#             except Exception as e:
#                 print(f"ì˜¤ë¥˜: ë°œí™” '{seg_id}'ì— ëŒ€í•œ ê·¸ë˜í”„ êµ¬ì¶• ì‹¤íŒ¨ - {e}")
#                 continue

#             # ì´ì „ ê·¸ë˜í”„ì™€ ë³‘í•©
#             video_graph = merge_graph(video_graph, current_graph)

#         # ì „ì²´ ë¹„ë””ì˜¤ ê·¸ë˜í”„ ìºì‹œ ì €ì¥
#         if video_graph:
#             output_path = cache_dir / _hash(f"video_graph::{video_id}")
#             torch.save(video_graph, output_path)
#             print(f"\nì„±ê³µ: video_id {video_id}ì— ëŒ€í•œ ìµœì¢… ê·¸ë˜í”„ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
#         # GraphBuilder ì¸ìŠ¤í„´ìŠ¤ ëª…ì‹œì ìœ¼ë¡œ ì‚­ì œí•˜ì—¬ ë©”ëª¨ë¦¬ í•´ì œ ìœ ë„
#         del graph_builder

#     conn.close()
#     print("\nëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# if __name__ == "__main__":
#     build_and_cache_graphs()

import pandas as pd
import sqlite3
import torch
import hashlib
import json
import re
from functools import lru_cache
from pathlib import Path
from typing import Dict, List, Tuple
from torch_geometric.data import Data
from transformers import BertTokenizerFast, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch.nn as nn
import os
import sys

# GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸ ë° ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# í•´ì‹œ í•¨ìˆ˜ ì •ì˜
def _hash(key: str) -> str:
    return hashlib.md5(key.encode()).hexdigest() + ".pt"

# Tensorë¥¼ ì§€ì •ëœ ë””ë°”ì´ìŠ¤ë¡œ ì˜®ê¸°ëŠ” í—¬í¼ í•¨ìˆ˜
def _to_device(data: Dict, device: torch.device) -> Dict:
    return {k: v.to(device) for k, v in data.items()}

# VRAM ë¡œê¹… í•¨ìˆ˜
def log_vram(stage: str, device: torch.device):
    if device.type == 'cuda':
        torch.cuda.synchronize()
        dev_id = device.index if isinstance(device, torch.device) else torch.cuda.current_device()
        alloc = torch.cuda.memory_allocated(dev_id) / 1024**2
        reserved = torch.cuda.memory_reserved(dev_id) / 1024**2
        peak_a = torch.cuda.max_memory_allocated(dev_id) / 1024**2
        peak_r = torch.cuda.max_memory_reserved(dev_id) / 1024**2
        wasted = reserved - alloc
        print(f"[{stage:15s}] alloc: {alloc:6.1f} MB | reserved: {reserved:6.1f} MB | "
              f"peak_alloc: {peak_a:6.1f} MB | peak_reserved: {peak_r:6.1f} MB | wasted: {wasted:6.1f} MB")
        torch.cuda.reset_peak_memory_stats(dev_id)

# Edge type id ì •ì˜
EDGE_TYPE: Dict[str, int] = {
    "t_t": 0,  # text  â†” text
    "v_v": 1,  # video â†” video
    "a_a": 2,  # audio â†” audio
    "t_v": 3,  # text  â†” video
    "t_a": 4,  # text  â†” audio
    "utt": 5,  # utterance â†” utterance (for merging)
}

# ì–‘ë°©í–¥ ì—£ì§€ ì¶”ê°€ í—¬í¼
def _add_bidir(
    src: int,
    dst: int,
    etype: int,
    edge_src: List[int],
    edge_dst: List[int],
    edge_type: List[int],
):
    edge_src.extend([src, dst])
    edge_dst.extend([dst, src])
    edge_type.extend([etype, etype])
    
# models/graph_builder.pyì˜ ì˜ì¡´ì„± í•´ê²°ì„ ìœ„í•´ í•„ìš”í•œ í´ë˜ìŠ¤ë“¤ì„ ì—¬ê¸°ì— í¬í•¨
class ExternalFinancialKnowledgeModel:
    def __init__(self,
                 wiki_db="data/wikidata_revisions.db",
                 speech_db="data/speech_segments.db"):
        self.wiki = sqlite3.connect(wiki_db); self.wiki.row_factory = sqlite3.Row
        self.speech = sqlite3.connect(speech_db); self.speech.row_factory = sqlite3.Row

        self.target_entities = self._load_target_entities()
        label_df = pd.read_sql("SELECT DISTINCT qid,value AS label FROM labels "
                               "WHERE lang='en';", self.wiki)
        label_df = label_df[label_df["label"].str.lower()
                               .isin([n.lower() for n in self.target_entities])]
        label_df["id_int"] = pd.factorize(label_df["qid"])[0]

        self.qid2int = dict(zip(label_df["qid"], label_df["id_int"]))
        self.int2qid = {v: k for k, v in self.qid2int.items()}  # ìˆ˜ì •ëœ ë¶€ë¶„ ë°˜ì˜
        self.label2qid = {l.lower(): q for q, l in zip(label_df["qid"],
                                                     label_df["label"])}

        safe = [re.sub(r"\s+", r"\\s+", re.escape(n.lower()))
                for n in self.target_entities]
        self._pat = re.compile(r"(" + "|".join(safe) + r")", re.I)

    def _load_target_entities(self) -> List[str]:
        df = pd.read_sql("SELECT persons_found FROM video_metadata;", self.speech)
        names = set()
        for js in df["persons_found"]:
            if js: names.update(json.loads(js).keys())
        return list(names)

    def identify_entities(self, text: str) -> List[str]:
        t = re.sub(r"[^\w\s]", "", text.lower())
        return list({m.strip() for m in self._pat.findall(t)})

    def entities_to_id(self, ents: List[str]) -> List[int]:
        return [self.qid2int[self.label2qid[e.lower()]]
                for e in ents if e.lower() in self.label2qid]
    
    @lru_cache(maxsize=32)
    def _graph_until(self, time_iso: str) -> Data:
        sql = ("SELECT c.qid subj,c.property pid,c.value_qid obj "
               "FROM claims c JOIN revisions r USING(qid,revision_id) "
               "WHERE r.timestamp<=?")
        df = pd.read_sql(sql, self.wiki, params=(time_iso,))
        df = df[df["subj"].isin(self.qid2int) & df["obj"].isin(self.qid2int)]
        if df.empty: return Data()

        src = torch.tensor(df["subj"].map(self.qid2int).to_numpy(), dtype=torch.long)
        dst = torch.tensor(df["obj"].map(self.qid2int).to_numpy(), dtype=torch.long)
        rel = torch.tensor(pd.factorize(df["pid"])[0], dtype=torch.long).view(-1, 1)
        return Data(edge_index=torch.stack([src, dst]), edge_attr=rel)
        
    def acquire_related_external_knowledge(
        self, text: str, time_iso: str,
        add_reverse=True, add_self_loop=True
    ) -> Tuple[List[int], Data]:
        ids = self.entities_to_id(self.identify_entities(text))
        G = self._graph_until(time_iso)
        if not ids or G.edge_index.numel() == 0: return ids, Data()

        mask = (torch.isin(G.edge_index[0], torch.tensor(ids)) |
                torch.isin(G.edge_index[1], torch.tensor(ids)))
        ei, ea = G.edge_index[:, mask], G.edge_attr[mask]

        if add_reverse:
            ei = torch.cat([ei, ei.flip(0)], 1)
            ea = torch.cat([ea, ea], 0)
        if add_self_loop:
            loops = torch.tensor(ids, dtype=torch.long, device=ei.device)
            ei = torch.cat([ei, loops.unsqueeze(0).repeat(2, 1)], 1)
            ea = torch.cat([ea, torch.full((len(loops), 1), -1,
                                           dtype=torch.long, device=ei.device)], 0)
        return ids, Data(edge_index=ei, edge_attr=ea)

# TextFeatureExtractor í´ë˜ìŠ¤ (ìˆ˜ì •ëœ ë‚´ìš© ë°˜ì˜)
def build_cross_map(wp_offsets: List[Tuple[int, int]],
                    glm_offsets: List[Tuple[int, int]]) -> List[List[int]]:
    mapping = [[] for _ in wp_offsets]; p = 0
    for i,(ws,we) in enumerate(wp_offsets):
        while p < len(glm_offsets) and glm_offsets[p][1] <= ws: p += 1
        q = p
        while q < len(glm_offsets) and glm_offsets[q][0] < we:
            mapping[i].append(q); q += 1
    return mapping

class TextFeatureExtractor:
    def __init__(self):
        self.wp_tok = BertTokenizerFast.from_pretrained("bert-base-uncased")
        self.glm_tok: AutoTokenizer = AutoTokenizer.from_pretrained(
            "THUDM/chatglm2-6b", trust_remote_code=True
        )
        self.fast_offset = True
        quantization_config = BitsAndBytesConfig(load_in_8bit=True)
        self.glm = AutoModelForCausalLM.from_pretrained(
            "THUDM/chatglm2-6b",
            trust_remote_code=True,
            quantization_config=quantization_config,
            output_hidden_states=True,
            device_map="auto"
        )
        self.proj_down = nn.Linear(4096, 768, bias=False).to(device).half()
        self.proj_up = nn.Linear(768, 4096, bias=False).to(device).half()
        self.norm = nn.LayerNorm(768).to(device).half()

        for m in (self.proj_down, self.proj_up):
            nn.init.xavier_uniform_(m.weight)

    def _manual_offsets(self, text: str, toks: List[str]):
        norm = text.lower(); p, off = 0, []
        for t in toks:
            tc = t.lstrip(" "); tc = tc if tc else " "
            j = norm.find(tc, p)
            j = j if j != -1 else p
            off.append((j, j + len(tc))); p = j + len(tc)
        return off

    @torch.no_grad()
    def encode(self,
               utterance_text: str,
               knowledge_triples: list | None = None,
               anchor_entities: list | None = None):
        know_parts = []
        if anchor_entities:
            know_parts.append(" [ENT] ".join(map(str, anchor_entities)))
        if knowledge_triples:
            know_parts += [f"{h} [R] {r} [T] {t}" for h, r, t in knowledge_triples]
        knowledge_text = " ".join(know_parts) if know_parts else None

        wp = self.wp_tok(
            utterance_text,
            text_pair=knowledge_text,
            return_offsets_mapping=True,
            return_token_type_ids=True,
            return_tensors="pt",
        )
        wp = _to_device(wp, self.glm.device)
        wp_tokens = self.wp_tok.convert_ids_to_tokens(wp["input_ids"][0])
        wp_offsets = wp["offset_mapping"][0].tolist()
        token_types = wp["token_type_ids"][0].tolist()
        sep_idx = [i for i, tok in enumerate(wp_tokens) if tok == "[SEP]"]

        knowledge_blocks: List[Tuple[int, int]] = []
        in_block, start = False, None
        for i, (tt, tok) in enumerate(zip(token_types, wp_tokens)):
            if tt == 1 and tok != "[SEP]":
                if not in_block:
                    start, in_block = i, True
            else:
                if in_block:
                    knowledge_blocks.append((start, i))
                    in_block = False
        if in_block:
            knowledge_blocks.append((start, len(wp_tokens)))

        merged = utterance_text + (" [SEP] " + knowledge_text if knowledge_text else "")
        glm_ids = self.glm_tok.encode(merged, add_special_tokens=False)
        glm_enc = {
            "input_ids": torch.tensor([glm_ids], device=self.glm.device),
            "attention_mask": torch.ones(
                1, len(glm_ids), dtype=torch.long, device=self.glm.device)
        }
        glm_tokens = self.glm_tok.convert_ids_to_tokens(glm_ids)
        glm_offsets = (self.glm_tok(
            merged, add_special_tokens=False, return_offsets_mapping=True
        )["offset_mapping"]
                       if hasattr(self.glm_tok, "get_offsets_mapping")
                       else self._manual_offsets(merged, glm_tokens))

        hidden4096 = self.glm(**glm_enc).hidden_states[-1][0]
        hid768 = self.norm(self.proj_down(hidden4096))

        map_wp2glm = build_cross_map(wp_offsets, glm_offsets)
        max_idx = hid768.size(0)
        wp_emb = torch.stack([
            (
                hid768[torch.tensor(valid, dtype=torch.long,
                                    device=hid768.device)].mean(0)
                if (valid := [i for i in ids if i < max_idx])
                else torch.zeros(768, device=hid768.device)
            )
            for ids in map_wp2glm
        ])

        meta = {
            "wp_tokens": wp_tokens,
            "glm_tokens": glm_tokens,
            "map_wp2glm": map_wp2glm,
            "sep": sep_idx,
            "knowledge_blocks": knowledge_blocks,
        }
        return wp_emb, meta

    def cleanup(self):
        print("TextFeatureExtractor ë©”ëª¨ë¦¬ í•´ì œ ì¤‘...")
        if hasattr(self, 'glm'):
            del self.glm
        if hasattr(self, 'proj_down'):
            del self.proj_down
        if hasattr(self, 'proj_up'):
            del self.proj_up
        torch.cuda.empty_cache()
        print("TextFeatureExtractor ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")

# GraphBuilder í´ë˜ìŠ¤
class GraphBuilder:
    """ë‹¨ì¼ ë°œí™” ê·¸ë˜í”„ + ì—¬ëŸ¬ ë°œí™” ë³‘í•© ìœ í‹¸ (***ì–‘ë°©í–¥ ì—£ì§€ ë²„ì „***)"""

    # [ìˆ˜ì •ë¨] ìƒì„±ìì—ì„œ time_iso ì œê±°
    def __init__(self, *, merge_anchor: bool = False):
        self.merge_anchor = merge_anchor
        self.text_enc = TextFeatureExtractor()
        self.ekm = ExternalFinancialKnowledgeModel()

    def __del__(self):
        if hasattr(self, 'text_enc'):
            self.text_enc.cleanup()
            del self.text_enc
        print("GraphBuilder ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
        log_vram("del", device)

    def _triple_to_string(self, subj: str, pid: str, obj: str) -> str:
        if not hasattr(self, "_pid_cache"):
            self._pid_cache: Dict[str, str] = {}
        if pid not in self._pid_cache:
            row = self.ekm.wiki.execute(
                "SELECT label FROM property_labels WHERE pid=? LIMIT 1", (pid,)
            ).fetchone()
            self._pid_cache[pid] = row[0] if row else pid
        return f"{subj} [REL] {self._pid_cache[pid].replace(' ', '_')} [REL] {obj}"

    # [ìˆ˜ì •ë¨] build ë©”ì†Œë“œê°€ time_isoë¥¼ ì¸ìë¡œ ë°›ë„ë¡ ë³€ê²½
    def build(
        self,
        utterance_text: str,
        time_iso: str,
        video_emb: torch.Tensor | None = None,
        audio_emb: torch.Tensor | None = None,
    ) -> Data:
        """ë°œí™”(í…ìŠ¤íŠ¸+ëª¨ë‹¬) â†’ PyG Data (ëª¨ë“  ì—£ì§€ ì–‘ë°©í–¥)"""
        if video_emb is not None and video_emb.is_cuda:
            video_emb = video_emb.cpu()
        if audio_emb is not None and audio_emb.is_cuda:
            audio_emb = audio_emb.cpu()

        # 1) ì™¸ë¶€ ì§€ì‹ íŠ¸ë¦¬í”Œ ìˆ˜ì§‘
        # [ìˆ˜ì •ë¨] self.time_iso ëŒ€ì‹  ì¸ìë¡œ ë°›ì€ time_iso ì‚¬ìš©
        ent_ids, ek_sub = self.ekm.acquire_related_external_knowledge(
            utterance_text, time_iso
        )
        triples: List[Tuple[str, str, str]] = []
        if ek_sub.num_edges:
            for s, d, (prop,) in zip(*ek_sub.edge_index, ek_sub.edge_attr):
                triples.append(
                    (
                        self.ekm.int2qid[s.item()],
                        f"P{prop.item()}",
                        self.ekm.int2qid[d.item()],
                    )
                )

        # 2) í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        wp_emb, meta = self.text_enc.encode(
            utterance_text,
            knowledge_triples=[self._triple_to_string(*t) for t in triples],
            anchor_entities=[self.ekm.int2qid[i] for i in ent_ids],
        )
        if wp_emb.is_cuda:
            wp_emb = wp_emb.cpu()
        hs = wp_emb
        D = hs.size(1)
        sep0 = meta["sep"][0] if meta["sep"] else -1

        node_feats: List[torch.Tensor] = []
        edge_src: List[int] = []
        edge_dst: List[int] = []
        edge_type: List[int] = []

        # --- utterance text í† í° ë…¸ë“œ ---
        text_token_map: Dict[int, int] = {
            idx: len(node_feats) for idx in range(1, sep0)
        }
        node_feats.extend([hs[i] for i in range(1, sep0)])
        text_nodes = list(text_token_map.values())
        for i in range(len(text_nodes) - 1):
            _add_bidir(
                text_nodes[i], text_nodes[i + 1], EDGE_TYPE["t_t"], edge_src, edge_dst, edge_type
            )

        # --- knowledge í† í° ë…¸ë“œ ---
        knowledge_token_map: Dict[int, int] = {}
        for s, e in meta["knowledge_blocks"]:
            prev = None
            for idx in range(s, e):
                g = len(node_feats)
                knowledge_token_map[idx] = g
                node_feats.append(hs[idx])
                if prev is not None:
                    _add_bidir(prev, g, EDGE_TYPE["t_t"], edge_src, edge_dst, edge_type)
                prev = g

        # ë¹„ë””ì˜¤ ë…¸ë“œ
        if video_emb is not None and video_emb.numel():
            v_idx = len(node_feats)
            node_feats.append(video_emb.squeeze(0))
            for t in text_nodes:
                _add_bidir(t, v_idx, EDGE_TYPE["t_v"], edge_src, edge_dst, edge_type)
        else:
            v_idx = -1

        # ì˜¤ë””ì˜¤ ë…¸ë“œ
        if audio_emb is not None and audio_emb.numel():
            a_idx = len(node_feats)
            node_feats.append(audio_emb.squeeze(0))
            for t in text_nodes:
                _add_bidir(t, a_idx, EDGE_TYPE["t_a"], edge_src, edge_dst, edge_type)
        else:
            a_idx = -1

        x = torch.stack(node_feats) if node_feats else torch.empty(0, D)
        edge_index = (
            torch.tensor([edge_src, edge_dst], dtype=torch.long)
            if edge_src
            else torch.empty(2, 0, dtype=torch.long)
        )
        edge_type_t = (
            torch.tensor(edge_type, dtype=torch.long)
            if edge_type
            else torch.empty(0, dtype=torch.long)
        )

        data = Data(x=x, edge_index=edge_index, edge_type=edge_type_t)
        data.node_meta = {
            "text_nodes": len(text_nodes),
            "knowledge_nodes": len(knowledge_token_map),
            "video_nodes": 1 if v_idx != -1 else 0,
            "audio_nodes": 1 if a_idx != -1 else 0,
            "triples": len(triples),
        }
        data.utt_meta = {
            "first_text_node": text_nodes[0] if text_nodes else -1,
            "last_text_node": text_nodes[-1] if text_nodes else -1,
        }
        return data

# models/graph_utils.pyì˜ merge_graph ë©”ì†Œë“œ
def merge_graph(prev_graph: Data | None, current_graph: Data) -> Data:
    if prev_graph is None or prev_graph.x.numel() == 0:
        return current_graph

    # ë…¸ë“œ í”¼ì²˜ ë³‘í•©
    x_merged = torch.cat([prev_graph.x, current_graph.x], dim=0)

    # ì—£ì§€ ì¸ë±ìŠ¤ ë° íƒ€ì… ë³‘í•© (ì˜¤í”„ì…‹ ì ìš©)
    num_prev_nodes = prev_graph.x.size(0)
    edge_index_current_offset = current_graph.edge_index + num_prev_nodes
    edge_index_merged = torch.cat([prev_graph.edge_index, edge_index_current_offset], dim=1)

    edge_type_merged = torch.cat([prev_graph.edge_type, current_graph.edge_type])

    # utterance ì—°ê²° ì—£ì§€ ì¶”ê°€ (ì–‘ë°©í–¥)
    if prev_graph.utt_meta["last_text_node"] != -1 and current_graph.utt_meta["first_text_node"] != -1:
        prev_last_node = prev_graph.utt_meta["last_text_node"]
        curr_first_node = current_graph.utt_meta["first_text_node"] + num_prev_nodes
        
        utt_edge_index = torch.tensor([[prev_last_node, curr_first_node],
                                       [curr_first_node, prev_last_node]], dtype=torch.long)
        utt_edge_type = torch.tensor([EDGE_TYPE["utt"], EDGE_TYPE["utt"]], dtype=torch.long)

        edge_index_merged = torch.cat([edge_index_merged, utt_edge_index], dim=1)
        edge_type_merged = torch.cat([edge_type_merged, utt_edge_type])

    # ë©”íƒ€ ì •ë³´ ë³‘í•©
    node_meta_merged = {k: prev_graph.node_meta.get(k, 0) + current_graph.node_meta.get(k, 0)
                        for k in set(prev_graph.node_meta.keys()) | set(current_graph.node_meta.keys())}
    
    utt_meta_merged = {
        "first_text_node": prev_graph.utt_meta["first_text_node"],
        "last_text_node": current_graph.utt_meta["last_text_node"] + num_prev_nodes
    }

    merged_graph = Data(x=x_merged, edge_index=edge_index_merged, edge_type=edge_type_merged)
    merged_graph.node_meta = node_meta_merged
    merged_graph.utt_meta = utt_meta_merged
    return merged_graph

def build_and_cache_graphs():
    """
    ready_videos.csvì— ìˆëŠ” video_idì— ëŒ€í•´ ê·¸ë˜í”„ë¥¼ êµ¬ì¶•í•˜ê³  ìºì‹œë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    ready_videos_path = "data/ready_videos.csv"
    speech_db_path = "data/speech_segments.db"
    cache_dir = Path("cache")
    cache_dir.mkdir(exist_ok=True)

    # 1. ready_videos.csvì—ì„œ video_id ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
    try:
        ready_videos_df = pd.read_csv(ready_videos_path)
        video_ids = ready_videos_df["video_id"].tolist()
        print(f"ì´ {len(video_ids)}ê°œì˜ video_idë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    except FileNotFoundError:
        print(f"ì˜¤ë¥˜: {ready_videos_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. speech_segments.db ì—°ê²°
    try:
        conn = sqlite3.connect(speech_db_path)
        conn.row_factory = sqlite3.Row
        print(f"{speech_db_path}ì— ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except sqlite3.Error as e:
        print(f"ì˜¤ë¥˜: {speech_db_path} ì—°ê²° ì‹¤íŒ¨ - {e}")
        return

    # [ì¶”ê°€ë¨] GraphBuilder ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë£¨í”„ ë°–ì—ì„œ í•œ ë²ˆë§Œ ìƒì„±
    print("GraphBuilder ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ëª¨ë¸ ë¡œë”©ìœ¼ë¡œ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤...")
    try:
        graph_builder = GraphBuilder()
    except Exception as e:
        print(f"ì˜¤ë¥˜: GraphBuilder ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨ - {e}")
        conn.close()
        return

    # 3. ê° video_idì— ëŒ€í•´ ê·¸ë˜í”„ êµ¬ì¶• ë° ìºì‹œ ì €ì¥
    for video_id in video_ids:
        # ğŸŸ¢ ì¶”ê°€ëœ ë¡œì§: ìµœì¢… ê·¸ë˜í”„ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        final_graph_path = cache_dir / _hash(f"video_graph::{video_id}")
        if final_graph_path.exists():
            print(f"\n[Video ID: {video_id}] ìµœì¢… ê·¸ë˜í”„ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
            
        print(f"\n[Video ID: {video_id}] ê·¸ë˜í”„ êµ¬ì¶• ì‹œì‘...")
        
        # ë¹„ë””ì˜¤ ì—…ë¡œë“œ ì‹œê°„ ì¡°íšŒ
        try:
            video_meta = conn.execute(
                "SELECT published_date FROM video_metadata WHERE video_id = ?",
                (video_id,)
            ).fetchone()
            if not video_meta:
                print(f"ê²½ê³ : video_id {video_id}ì— ëŒ€í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
            upload_time = video_meta["published_date"]
        except sqlite3.Error as e:
            print(f"ì˜¤ë¥˜: video_id {video_id}ì˜ ì—…ë¡œë“œ ì‹œê°„ ì¡°íšŒ ì‹¤íŒ¨ - {e}")
            continue

        # [ìˆ˜ì •ë¨] ë£¨í”„ ë‚´ì—ì„œ GraphBuilderë¥¼ ìƒì„±í•˜ëŠ” ëŒ€ì‹ , ì´ë¯¸ ìƒì„±ëœ ê°ì²´ ì‚¬ìš©
        # graph_builder = GraphBuilder(time_iso=upload_time) # ì´ ë¼ì¸ ì œê±°

        video_graph = None

        # ë¹„ë””ì˜¤ì— ëŒ€í•œ ë°œí™” ë¶ˆëŸ¬ì˜¤ê¸°
        speech_segments = conn.execute(
            "SELECT segment_id, script FROM speech_segments WHERE video_id = ? ORDER BY start_time",
            (video_id,)
        ).fetchall()
        
        if not speech_segments:
            print(f"ê²½ê³ : video_id {video_id}ì— ëŒ€í•œ ë°œí™”ê°€ ì—†ìŠµë‹ˆë‹¤. ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            continue
        
        for segment in speech_segments:
            seg_id = segment["segment_id"]
            utterance_text = segment["script"]
            
            # [ì¶”ê°€ë¨] ë¹ˆ ë°œí™” í…ìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°
            if not utterance_text or not utterance_text.strip():
                print(f" - ê²½ê³ : ë°œí™” '{seg_id}'ì˜ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
                
            print(f" - ë°œí™” '{seg_id}' ì²˜ë¦¬ ì¤‘...")

            # ìºì‹œ íŒŒì¼ ê²½ë¡œ ì„¤ì •
            video_key = f"vid::{seg_id}"
            audio_key = f"aud::{seg_id}"
            video_cache_path = cache_dir / _hash(video_key)
            audio_cache_path = cache_dir / _hash(audio_key)

            # ì„ë² ë”© í…ì„œ ë¡œë“œ
            video_emb, audio_emb = None, None
            try:
                if video_cache_path.exists():
                    video_emb = torch.load(video_cache_path)
                if audio_cache_path.exists():
                    audio_emb = torch.load(audio_cache_path)
            except Exception as e:
                print(f"ì˜¤ë¥˜: ë°œí™” '{seg_id}'ì˜ ìºì‹œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ - {e}")
                continue

            # ë‹¨ì¼ ë°œí™” ê·¸ë˜í”„ êµ¬ì¶•
            try:
                # [ìˆ˜ì •ë¨] build ë©”ì†Œë“œì— time_iso ì¸ì ì „ë‹¬
                current_graph = graph_builder.build(
                    utterance_text=utterance_text,
                    time_iso=upload_time, 
                    video_emb=video_emb,
                    audio_emb=audio_emb
                )
            except Exception as e:
                print(f"ì˜¤ë¥˜: ë°œí™” '{seg_id}'ì— ëŒ€í•œ ê·¸ë˜í”„ êµ¬ì¶• ì‹¤íŒ¨ - {e}")
                continue

            # ì´ì „ ê·¸ë˜í”„ì™€ ë³‘í•©
            video_graph = merge_graph(video_graph, current_graph)

        # ì „ì²´ ë¹„ë””ì˜¤ ê·¸ë˜í”„ ìºì‹œ ì €ì¥
        if video_graph:
            # ğŸŸ¢ ì¶”ê°€ëœ ë¡œì§: ì´ì „ì— ì¡´ì¬í•˜ì§€ ì•Šì•˜ë˜ ê²½ìš°ì—ë§Œ ì €ì¥
            torch.save(video_graph, final_graph_path)
            print(f"\nì„±ê³µ: video_id {video_id}ì— ëŒ€í•œ ìµœì¢… ê·¸ë˜í”„ê°€ {final_graph_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # [ìˆ˜ì •ë¨] ë£¨í”„ ë‚´ì—ì„œ del graph_builder ì œê±°

    # [ì¶”ê°€ë¨] ëª¨ë“  ì‘ì—…ì´ ëë‚œ í›„ GraphBuilder ì¸ìŠ¤í„´ìŠ¤ ëª…ì‹œì ìœ¼ë¡œ ì‚­ì œ
    print("\nëª¨ë“  ë¹„ë””ì˜¤ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. GraphBuilder ë¦¬ì†ŒìŠ¤ë¥¼ í•´ì œí•©ë‹ˆë‹¤.")
    del graph_builder
    torch.cuda.empty_cache()

    conn.close()
    print("\nëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    build_and_cache_graphs()