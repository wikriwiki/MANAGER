{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a94ce028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsl/anaconda3/envs/chatglm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'transformers' from '/home/dsl/anaconda3/envs/chatglm/lib/python3.11/site-packages/transformers/__init__.py'> \n",
      " /home/dsl/anaconda3/envs/chatglm/lib/python3.11/site-packages/transformers/__init__.py\n",
      "<module 'tokenizers' from '/home/dsl/anaconda3/envs/chatglm/lib/python3.11/site-packages/tokenizers/__init__.py'> \n",
      " /home/dsl/anaconda3/envs/chatglm/lib/python3.11/site-packages/tokenizers/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import transformers, tokenizers, inspect, os\n",
    "print(transformers, \"\\n\", transformers.__file__)\n",
    "print(tokenizers,   \"\\n\", tokenizers.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42c600e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsl/anaconda3/envs/chatglm/lib/python3.11/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/dsl/anaconda3/envs/chatglm/lib/python3.11/site-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/home/dsl/anaconda3/envs/chatglm/lib/python3.11/site-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /home/dsl/anaconda3/envs/chatglm/lib/python3.11/site-packages/torch_cluster/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
      "/home/dsl/anaconda3/envs/chatglm/lib/python3.11/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /home/dsl/anaconda3/envs/chatglm/lib/python3.11/site-packages/torch_spline_conv/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(\n",
      "/home/dsl/anaconda3/envs/chatglm/lib/python3.11/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /home/dsl/anaconda3/envs/chatglm/lib/python3.11/site-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 모듈 임포트 완료 및 경로 설정 완료.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm.auto import tqdm\n",
    "import sqlite3\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "# 프로젝트 루트 디렉토리를 Python 경로에 추가합니다.\n",
    "# 이렇게 해야 dataset, models 등 서브 디렉토리의 모듈을 직접 임포트할 수 있습니다.\n",
    "# 현재 Jupyter Notebook이 MANAGER 디렉토리 내에서 실행된다고 가정합니다.\n",
    "project_root = Path(os.getcwd())\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# 커스텀 모듈 임포트\n",
    "from dataset.data import VideoPersonDataset\n",
    "from models.manager_graphtokens import GraphTokenManager\n",
    "from experiments import init_experiment_db, insert_new_experiment, update_experiment_metrics, insert_sample_predictions\n",
    "# train.py, validate.py, eval.py의 핵심 함수를 직접 임포트합니다.\n",
    "# argparse 없이 함수 인자로 직접 값을 전달할 수 있게 됩니다.\n",
    "from train import seed_all, get_dataloaders, evaluate as train_evaluate # train.py의 evaluate는 이름 충돌 피하기 위해 train_evaluate로 변경\n",
    "from validate import run_eval as validate_run_eval, load_ckpt as validate_load_ckpt, get_loader as validate_get_loader\n",
    "from eval import run_test as eval_run_test, load_ckpt as eval_load_ckpt # eval.py의 load_ckpt도 이름 충돌 피하기 위해 변경\n",
    "\n",
    "print(\"모든 모듈 임포트 완료 및 경로 설정 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b52ad59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실험 데이터베이스 'experiment_results.db' 초기화 및 연결 완료.\n",
      "새로운 실험이 ID: 2로 시작되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 실험 설정 (하이퍼파라미터 및 경로)\n",
    "class ExperimentArgs:\n",
    "    def __init__(self):\n",
    "        self.db = \"data/speech_segments.db\" # 데이터베이스 경로\n",
    "        self.cache = \"cache/\" # 캐시 디렉토리\n",
    "        self.frames = \"data/frames/\" # 비디오 프레임 경로\n",
    "        self.wav = \"data/wav/\" # 오디오 파일 경로\n",
    "        self.ckpt_dir = \"checkpoints/\" # 체크포인트 저장 디렉토리\n",
    "        self.epochs = 5 # 훈련 에포크 수\n",
    "        self.lr = 2e-4 # 학습률\n",
    "        self.seed = 42 # 랜덤 시드\n",
    "        self.max_samples = 100 # 디버깅용 샘플 제한 (전체 데이터 사용 시 None)\n",
    "        self.output_csv = \"results_test.csv\" # eval.py에서 개별 예측 저장할 CSV\n",
    "        self.output_json = \"metrics_test.json\" # eval.py에서 최종 메트릭 저장할 JSON\n",
    "\n",
    "args = ExperimentArgs() # 인자 객체 생성\n",
    "\n",
    "# 체크포인트 디렉토리 생성\n",
    "Path(args.ckpt_dir).mkdir(exist_ok=True)\n",
    "\n",
    "# 실험 데이터베이스 초기화 및 연결\n",
    "experiment_db_path = \"experiment_results.db\" # 실험 결과를 저장할 DB 파일명\n",
    "exp_conn = init_experiment_db(db_path=experiment_db_path)\n",
    "print(f\"실험 데이터베이스 '{experiment_db_path}' 초기화 및 연결 완료.\")\n",
    "\n",
    "# 새 실험 레코드 삽입 (초기 정보)\n",
    "experiment_id = insert_new_experiment(exp_conn, args)\n",
    "print(f\"새로운 실험이 ID: {experiment_id}로 시작되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b9ec2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 장치: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsl/anaconda3/envs/chatglm/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.06s/it]\n",
      "/home/dsl/anaconda3/envs/chatglm/lib/python3.11/site-packages/transformers/utils/deprecation.py:165: UserWarning: The following named arguments are not valid for `BeitImageProcessor.__init__` and were ignored: 'feature_extractor_type'\n",
      "  return func(*args, **kwargs)\n",
      "Some weights of the model checkpoint at facebook/hubert-base-ls960 were not used when initializing HubertModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertModel were not initialized from the model checkpoint at facebook/hubert-base-ls960 and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/dsl/anaconda3/envs/chatglm/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.03s/it]\n",
      "/home/dsl/anaconda3/envs/chatglm/lib/python3.11/site-packages/transformers/utils/deprecation.py:165: UserWarning: The following named arguments are not valid for `BeitImageProcessor.__init__` and were ignored: 'feature_extractor_type'\n",
      "  return func(*args, **kwargs)\n",
      "Some weights of the model checkpoint at facebook/hubert-base-ls960 were not used when initializing HubertModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertModel were not initialized from the model checkpoint at facebook/hubert-base-ls960 and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플: 299, 검증 샘플: 306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.07s/it]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MatmulLtState' object has no attribute 'memory_efficient_backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m훈련 샘플: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader.dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, 검증 샘플: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(val_loader.dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# 모델 로드 및 GPU 이동\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m model = \u001b[43mGraphTokenManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m model.train()\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# 옵티마이저 설정\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MANAGER/models/manager_graphtokens.py:61\u001b[39m, in \u001b[36mGraphTokenManager.__init__\u001b[39m\u001b[34m(self, glm_ckpt, gcn_layers, lora_r, lora_alpha, lora_dropout)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mself\u001b[39m.glm = AutoModel.from_pretrained(\n\u001b[32m     49\u001b[39m     glm_ckpt,\n\u001b[32m     50\u001b[39m     torch_dtype=torch.float16,\n\u001b[32m   (...)\u001b[39m\u001b[32m     53\u001b[39m     device_map=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     54\u001b[39m )\n\u001b[32m     55\u001b[39m lora_cfg = LoraConfig(\n\u001b[32m     56\u001b[39m     r=lora_r,\n\u001b[32m     57\u001b[39m     lora_alpha=lora_alpha,\n\u001b[32m     58\u001b[39m     lora_dropout=lora_dropout,\n\u001b[32m     59\u001b[39m     target_modules=[\u001b[33m\"\u001b[39m\u001b[33mquery_key_value\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     60\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28mself\u001b[39m.glm = \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mglm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# 분류 헤드를 device로 이동\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;28mself\u001b[39m.cls_head = nn.Linear(\u001b[32m4096\u001b[39m, \u001b[32m1\u001b[39m).to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chatglm/lib/python3.11/site-packages/peft/mapping.py:103\u001b[39m, in \u001b[36mget_peft_model\u001b[39m\u001b[34m(model, peft_config, adapter_name)\u001b[39m\n\u001b[32m    100\u001b[39m peft_config.base_model_name_or_path = model.\u001b[34m__dict__\u001b[39m.get(\u001b[33m\"\u001b[39m\u001b[33mname_or_path\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m peft_config.task_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING.keys() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m peft_config.is_prompt_learning:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPeftModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m peft_config.is_prompt_learning:\n\u001b[32m    105\u001b[39m     peft_config = _prepare_prompt_learning_config(peft_config, model_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chatglm/lib/python3.11/site-packages/peft/peft_model.py:111\u001b[39m, in \u001b[36mPeftModel.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m peft_config.is_prompt_learning:\n\u001b[32m    110\u001b[39m     \u001b[38;5;28mself\u001b[39m.peft_config[adapter_name] = peft_config\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28mself\u001b[39m.base_model = \u001b[43mPEFT_TYPE_TO_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpeft_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_additional_trainable_modules(peft_config, adapter_name)\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chatglm/lib/python3.11/site-packages/peft/tuners/lora.py:274\u001b[39m, in \u001b[36mLoraModel.__init__\u001b[39m\u001b[34m(self, model, config, adapter_name)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chatglm/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:88\u001b[39m, in \u001b[36mBaseTuner.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     86\u001b[39m     \u001b[38;5;28mself\u001b[39m.config = {\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcustom\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28mself\u001b[39m.model.peft_config = \u001b[38;5;28mself\u001b[39m.peft_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chatglm/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:219\u001b[39m, in \u001b[36mBaseTuner.inject_adapter\u001b[39m\u001b[34m(self, model, adapter_name)\u001b[39m\n\u001b[32m    212\u001b[39m     parent, target, target_name = _get_submodules(model, key)\n\u001b[32m    214\u001b[39m     optionnal_kwargs = {\n\u001b[32m    215\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mloaded_in_8bit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33mis_loaded_in_8bit\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m    216\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mloaded_in_4bit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33mis_loaded_in_4bit\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m    217\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcurrent_key\u001b[39m\u001b[33m\"\u001b[39m: key,\n\u001b[32m    218\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_and_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptionnal_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_target_modules_in_base_model:\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    223\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTarget modules \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config.target_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in the base model. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease check the target modules and try again.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    225\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chatglm/lib/python3.11/site-packages/peft/tuners/lora.py:372\u001b[39m, in \u001b[36mLoraModel._create_and_replace\u001b[39m\u001b[34m(self, lora_config, adapter_name, target, target_name, parent, **optionnal_kwargs)\u001b[39m\n\u001b[32m    364\u001b[39m     target.update_layer(\n\u001b[32m    365\u001b[39m         adapter_name,\n\u001b[32m    366\u001b[39m         lora_config.r,\n\u001b[32m   (...)\u001b[39m\u001b[32m    369\u001b[39m         lora_config.init_lora_weights,\n\u001b[32m    370\u001b[39m     )\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     new_module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    373\u001b[39m     \u001b[38;5;28mself\u001b[39m._replace_module(parent, target_name, new_module, target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chatglm/lib/python3.11/site-packages/peft/tuners/lora.py:428\u001b[39m, in \u001b[36mLoraModel._create_new_module\u001b[39m\u001b[34m(lora_config, adapter_name, target, **kwargs)\u001b[39m\n\u001b[32m    423\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loaded_in_8bit \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target, bnb.nn.Linear8bitLt):\n\u001b[32m    424\u001b[39m     eightbit_kwargs = kwargs.copy()\n\u001b[32m    425\u001b[39m     eightbit_kwargs.update(\n\u001b[32m    426\u001b[39m         {\n\u001b[32m    427\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhas_fp16_weights\u001b[39m\u001b[33m\"\u001b[39m: target.state.has_fp16_weights,\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmemory_efficient_backward\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mtarget\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmemory_efficient_backward\u001b[49m,\n\u001b[32m    429\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mthreshold\u001b[39m\u001b[33m\"\u001b[39m: target.state.threshold,\n\u001b[32m    430\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: target.index,\n\u001b[32m    431\u001b[39m         }\n\u001b[32m    432\u001b[39m     )\n\u001b[32m    433\u001b[39m     new_module = Linear8bitLt(\n\u001b[32m    434\u001b[39m         adapter_name, target.in_features, target.out_features, bias=bias, **eightbit_kwargs\n\u001b[32m    435\u001b[39m     )\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m loaded_in_4bit \u001b[38;5;129;01mand\u001b[39;00m is_bnb_4bit_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target, bnb.nn.Linear4bit):\n",
      "\u001b[31mAttributeError\u001b[39m: 'MatmulLtState' object has no attribute 'memory_efficient_backward'"
     ]
    }
   ],
   "source": [
    "# 훈련 장치 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"훈련 장치: {device}\")\n",
    "\n",
    "# 랜덤 시드 설정\n",
    "seed_all(args.seed)\n",
    "\n",
    "# 데이터 로더 준비\n",
    "train_loader, val_loader = get_dataloaders(\n",
    "    db=args.db, cache=args.cache, frames=args.frames, wav=args.wav,\n",
    "    batch=1, max_samples=args.max_samples, seed=args.seed\n",
    ")\n",
    "print(f\"훈련 샘플: {len(train_loader.dataset)}, 검증 샘플: {len(val_loader.dataset)}\")\n",
    "\n",
    "# 모델 로드 및 GPU 이동\n",
    "model = GraphTokenManager()\n",
    "model.train()\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optim = torch.optim.AdamW(\n",
    "    model.parameters(), lr=args.lr, betas=(0.9,0.95), eps=1e-8\n",
    ")\n",
    "scaler = GradScaler()\n",
    "\n",
    "best_f1 = 0.0\n",
    "current_best_ckpt_path = None # 최적 체크포인트 경로 추적\n",
    "\n",
    "print(\"모델 훈련 시작...\")\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch} (Train)\")\n",
    "    for sample in pbar:\n",
    "        g = sample[\"graph\"].to(device)\n",
    "        lbl = sample[\"label\"].float().unsqueeze(0).to(device)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        with autocast():\n",
    "            logit, loss = model(g, sample[\"person\"][0], lbl)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optim)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optim); scaler.update()\n",
    "\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # ── 검증 (Validation) ──\n",
    "    # train.py의 evaluate 함수는 F1만 반환하지만, precision, recall도 필요하므로\n",
    "    # validate.py의 run_eval을 사용하거나 train_evaluate를 수정해야 합니다.\n",
    "    # 여기서는 validate.py의 run_eval을 사용하겠습니다.\n",
    "    val_metrics = validate_run_eval(val_loader, model, device) # run_eval은 dict 반환\n",
    "\n",
    "    print(f\"Epoch {epoch} Val Metrics: {json.dumps(val_metrics, indent=2)}\")\n",
    "\n",
    "    # DB에 검증 성능 업데이트\n",
    "    update_experiment_metrics(exp_conn, experiment_id, \"val\", val_metrics)\n",
    "    print(f\"실험 {experiment_id}의 검증 메트릭이 DB에 업데이트되었습니다.\")\n",
    "\n",
    "    # 최적 모델 저장\n",
    "    if val_metrics['f1'] > best_f1:\n",
    "        best_f1 = val_metrics['f1']\n",
    "        current_best_ckpt_path = Path(args.ckpt_dir) / f\"best_exp{experiment_id}.pt\" # 실험 ID를 포함하여 저장\n",
    "        torch.save({\n",
    "            \"gcn\" : model.gcn.state_dict(),\n",
    "            \"proj\": model.proj_up.state_dict(),\n",
    "            \"lora\": model.glm.state_dict(),\n",
    "            \"optim\": optim.state_dict(),\n",
    "        }, current_best_ckpt_path)\n",
    "        print(f\"  ✔ 새로운 최적 체크포인트 저장됨 (F1 {best_f1:.4f} at {current_best_ckpt_path})\")\n",
    "\n",
    "# 최종 체크포인트 경로를 DB에 업데이트 (best.pt가 아닌 고유 경로)\n",
    "if current_best_ckpt_path:\n",
    "    # 이전에 best_f1 업데이트 시 checkpoint_path도 함께 업데이트 되도록 experiments.py 수정했으므로 중복될 수 있습니다.\n",
    "    # 필요하다면 여기서 한번 더 최종 업데이트를 강제할 수 있습니다.\n",
    "    pass # 이미 위에서 F1 업데이트 시 경로도 업데이트되므로 생략\n",
    "else:\n",
    "    print(\"최적 체크포인트가 저장되지 않았습니다.\")\n",
    "\n",
    "print(\"모델 훈련 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7018d447",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n테스트 세트 평가 시작...\")\n",
    "\n",
    "# 최적 체크포인트 경로가 없으면 에러 또는 기본값 사용\n",
    "if not current_best_ckpt_path or not current_best_ckpt_path.exists():\n",
    "    print(f\"[WARN] 최적 체크포인트 '{current_best_ckpt_path}'를 찾을 수 없습니다. 기본 체크포인트 사용 시도.\")\n",
    "    # 실제 환경에서는 여기서 오류 처리하거나 기본 ckpt 경로로 대체해야 합니다.\n",
    "    # 예시: current_best_ckpt_path = Path(args.ckpt_dir) / \"best.pt\"\n",
    "\n",
    "if current_best_ckpt_path and current_best_ckpt_path.exists():\n",
    "    # 모델 재로드 (최적 체크포인트 로드)\n",
    "    test_model = GraphTokenManager().half().to(device)\n",
    "    eval_load_ckpt(test_model, current_best_ckpt_path) # eval.py의 load_ckpt 함수 사용\n",
    "    print(f\"테스트 평가를 위해 모델에 '{current_best_ckpt_path}' 체크포인트 로드 완료.\")\n",
    "\n",
    "    # 테스트 데이터 로더 준비\n",
    "    test_loader = validate_get_loader( # validate.py의 get_loader 사용\n",
    "        db=args.db, cache=args.cache, frames=args.frames, wav=args.wav,\n",
    "        split=\"test\", max_samples=args.max_samples, seed=args.seed\n",
    "    )\n",
    "    print(f\"테스트 샘플: {len(test_loader.dataset)}\")\n",
    "\n",
    "    # 최종 평가 실행 (eval.py의 run_test 사용)\n",
    "    # run_test는 메트릭과 개별 예측 리스트를 반환합니다.\n",
    "    final_metrics, individual_predictions_list = eval_run_test(test_loader, test_model, device, Path(args.output_csv))\n",
    "    print(f\"\\n최종 테스트 메트릭: {json.dumps(final_metrics, indent=2)}\")\n",
    "\n",
    "    # DB에 최종 테스트 성능 업데이트\n",
    "    update_experiment_metrics(exp_conn, experiment_id, \"test\", final_metrics)\n",
    "    print(f\"실험 {experiment_id}의 최종 테스트 메트릭이 DB에 업데이트되었습니다.\")\n",
    "\n",
    "    # 개별 샘플 예측 결과 DB에 저장\n",
    "    insert_sample_predictions(exp_conn, experiment_id, individual_predictions_list, \"test\")\n",
    "    print(f\"{len(individual_predictions_list)}개의 개별 샘플 예측 결과가 DB에 저장되었습니다.\")\n",
    "\n",
    "    # 최종 메트릭 JSON 파일 저장\n",
    "    Path(args.output_json).write_text(json.dumps(final_metrics, indent=2))\n",
    "    print(f\"최종 결과가 {args.output_csv} 및 {args.output_json}에 저장되었습니다.\")\n",
    "\n",
    "else:\n",
    "    print(\"[ERROR] 테스트 평가를 위한 최적 체크포인트를 찾을 수 없습니다. 평가를 건너뜝니다.\")\n",
    "\n",
    "# 데이터베이스 연결 종료\n",
    "exp_conn.close()\n",
    "print(\"모든 실험 과정 완료 및 데이터베이스 연결 종료.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatglm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
